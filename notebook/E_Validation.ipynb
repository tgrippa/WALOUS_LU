{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong><font size=\"6\">WALOUS</font></strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong><font size=\"6\">Land Use Classification</font></strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong><font size=\"6\">Validation</font></strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python code implement the method developed by ANAGEO (ULB). \n",
    "\n",
    "Code developped on Linux Mint 18.1 (Ubuntu Xenial 16.04), PostgreSQL 9.6.3, PostGIS 2.4.4 (r16526), GRASS GIS 7.3.svn (r71315) and GDAL 1.10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PostgreSQL installed on local computer or remote server, with PostGIS installed. A database should already have been created and postgis extension created on it.\n",
    "- \"shp2pgsql\" program which should already be installed by postgis installation process. See [this quick guide](http://www.bostongis.com/pgsql2shp_shp2pgsql_quickguide.bqg) for more information on the use of this program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is a Javascript section of code for building the Jupyter notebook's table of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define working environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries needed for setting parameters of operating system \n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import tempfile\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Psycopg2 library (interection with postgres database)\n",
    "import psycopg2\n",
    "## Import Subprocess\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Pandas library (View and manipulaiton of tables)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import pandas.io.sql as sqlio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import multiprocessing and functools libraries\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add folder with SCR provided belong to this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add local module to the path\n",
    "src = os.path.abspath('../SRC')\n",
    "if src not in sys.path:\n",
    "    sys.path.append(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup environment variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please edit the file in `../SRC/config.py`, containing the configuration parameters, according to your own computer setup. The following cell is used to run this file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run ../SRC/config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(config_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions that setup the environmental variables\n",
    "import environ_variables as envi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set environmental variables\n",
    "envi.setup_environmental_variables() \n",
    "# Display current environment variables of your computer\n",
    "envi.print_environmental_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions for processing time information\n",
    "import time\n",
    "from processing_time import start_processing, print_processing_time\n",
    "# Import function that check and create folder\n",
    "from mkdir import check_create_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom functions: Psycopg2 and Postgresql functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function that display postgresql's table header\n",
    "from display_header import display_header\n",
    "# Import function to creation connection to Postgresql database \n",
    "from postgres_functions import create_pg_connexion\n",
    "# Import function to creation of Postgresql schema \n",
    "from postgres_functions import create_pg_schema\n",
    "# Import function to give rights to user on a specific schema\n",
    "from postgres_functions import grant_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function that manage importation of a Shapefile into postgresql database\n",
    "from postgres_import import shp2pgsql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new directory for validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check and create folder if needed\n",
    "check_create_dir(config_parameters['validationfolder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new schema and import validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connexion to postgres database\n",
    "con = create_pg_connexion(config_parameters)\n",
    "# Create new schema\n",
    "create_pg_schema(con, 'validation', overwrite=False)\n",
    "grant_user(con, 'validation', 'bbeaumont')\n",
    "# Close connexion to postgres database\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import shapefile into postgis database\n",
    "shp2pgsql(data['validation'], 'validation', config_parameters, from_srid='31370', to_srid='31370', \n",
    "          create_opt='-d', psql_stdout=True, quiet=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create table with reference label and predicted label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __main__ import *\n",
    "import sys\n",
    "import psycopg2\n",
    "import time\n",
    "from processing_time import start_processing, print_processing_time\n",
    "   \n",
    "def CreateTableReferencePredicted(con, ref_schema, ref_table, pred_schema, pred_table):\n",
    "    '''\n",
    "    Function to create table with both reference label and classification prediction  \n",
    "    \n",
    "    Args: \n",
    "    'con' \n",
    "    'result_table_schema' \n",
    "    'result_table_name' \n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    '''\n",
    "    try:\n",
    "        # Time at starting\n",
    "        begintime = time.time() \n",
    "        # Create cursor\n",
    "        cursor = con.cursor()\n",
    "        # Drop table if exists\n",
    "        query = 'DROP TABLE IF EXISTS %s.valid_pred_ref;'%(ref_schema)\n",
    "        print(query + \"\\n\")\n",
    "        cursor.execute(query)\n",
    "        con.commit()\n",
    "        # Subquery\n",
    "        subquery = \"SELECT a.geom, a.capakey, walousmajv, b.walousmaj, \\\n",
    "        left(a.walousmajv,1) as ref_l1, left(a.walousmajv,3) as ref_l2, \\\n",
    "        a.codesecond, a.certitude, a.proportion, \\\n",
    "        left(b.walousmaj,1) as pred_l1, left(b.walousmaj,3) as pred_l2 \\\n",
    "        FROM %s.%s AS a LEFT JOIN %s.%s AS b \\\n",
    "        ON a.capakey = b.capakey\"%(ref_schema, ref_table, pred_schema, pred_table)\n",
    "        # Create table\n",
    "        query = 'CREATE TABLE %s.valid_pred_ref AS(%s);'%(ref_schema,subquery)\n",
    "        print(query + \"\\n\")\n",
    "        cursor.execute(query)\n",
    "        con.commit()\n",
    "        # Add columns\n",
    "        queries = []\n",
    "        queries.append('ALTER TABLE %s.valid_pred_ref ADD COLUMN IF NOT EXISTS agreement_l1 text'%ref_schema)\n",
    "        queries.append('ALTER TABLE %s.valid_pred_ref ADD COLUMN IF NOT EXISTS agreement_l2 text'%ref_schema)\n",
    "        print(\";\\n\".join(queries)+\";\\n\")\n",
    "        cursor.execute(\"; \".join(queries))\n",
    "        con.commit() \n",
    "        # Update column\n",
    "        queries = []\n",
    "        queries.append(\"UPDATE %s.valid_pred_ref SET ref_l2 = NULL WHERE LENGTH(ref_l2) < 3\"%ref_schema)\n",
    "        queries.append(\"UPDATE %s.valid_pred_ref SET pred_l2 = NULL WHERE LENGTH(pred_l2) < 3\"%ref_schema)\n",
    "        queries.append(\"UPDATE %s.valid_pred_ref SET agreement_l1 = \\\n",
    "        CASE WHEN ref_l1 = pred_l1 THEN True ELSE False END\"%ref_schema)\n",
    "        queries.append(\"UPDATE %s.valid_pred_ref SET agreement_l2 = \\\n",
    "        CASE WHEN ref_l2 = pred_l2 THEN True ELSE \\\n",
    "        CASE WHEN ref_l2 IS NOT NULL AND pred_l2 IS NOT NULL THEN False ELSE NULL END END\"%ref_schema)\n",
    "        print(\";\\n\".join(queries)+\";\\n\")\n",
    "        cursor.execute(\"; \".join(queries))\n",
    "        con.commit()          \n",
    "        ## Print processing time\n",
    "        print(print_processing_time(begintime, \"Creation of table with reference and prediction achieved in \"))\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        sys.exit(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connexion to postgres database\n",
    "con = create_pg_connexion(config_parameters)\n",
    "# Create table with \n",
    "CreateTableReferencePredicted(con, 'validation', data['validation'][0], 'validation', 'walousmaj_stratif_l1_200pt')\n",
    "# Close connexion to postgres database\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import sys\n",
    "import psycopg2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def GetAccuracyMeasure(con, schema, table, ref_column, pred_column, \n",
    "                       classes, output_folder, condition=None, weight=False):\n",
    "    \n",
    "    def plot_confusion_matrix(cm, classes,\n",
    "                              normalize=False,\n",
    "                              title='Confusion matrix',\n",
    "                              cmap=plt.cm.Blues):\n",
    "        \"\"\"\n",
    "        This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "        \"\"\"\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, ha='right', rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            print(\"Normalized confusion matrix\")\n",
    "        else:\n",
    "            print('Confusion matrix, without normalization')\n",
    "\n",
    "        #print(cm)\n",
    "\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            if normalize:\n",
    "                plt.text(j, i, round(cm[i, j],2),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "            else:\n",
    "                plt.text(j, i, cm[i, j],\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "    def GetRefPredLists(con, schema, table, ref_column, pred_column, condition):\n",
    "        '''\n",
    "        Function that return lists with reference label and prediction labels\n",
    "\n",
    "        Args: \n",
    "        'con' \n",
    "        'result_table_schema' \n",
    "        'result_table_name' \n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        try:\n",
    "            # Query\n",
    "            query = \"SELECT %s, %s, ROUND(ST_Area(geom)) FROM %s.%s \"%(ref_column, pred_column, schema, table)\n",
    "            if condition:\n",
    "                query += \"WHERE %s\"%condition  \n",
    "            cursor = con.cursor()\n",
    "            cursor.execute(query)\n",
    "            con.commit()\n",
    "            return zip(*cursor.fetchall())\n",
    "            cursor.close()\n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            sys.exit(error)               \n",
    "    \n",
    "    # Check and create folder if needed\n",
    "    check_create_dir(output_folder)\n",
    "    \n",
    "    # Get list with reference labels, prediction labels and area for weighting samples \n",
    "    ref_list, pred_list, area_list = GetRefPredLists(con,schema,table,ref_column,pred_column,condition)\n",
    "    \n",
    "    ##### Confusion matrix #####\n",
    "    # Compute confusion matrix\n",
    "    if weight:\n",
    "        cnf_matrix = confusion_matrix(ref_list, pred_list, sample_weight=area_list)\n",
    "    else:\n",
    "        cnf_matrix = confusion_matrix(ref_list, pred_list)\n",
    "    ## Export the row confusion matrix to output folder\n",
    "    output_rowconfmat = os.path.join(output_folder,\"rowconfusionmatrix.txt\")\n",
    "    np.savetxt(output_rowconfmat, cnf_matrix.astype(np.int), fmt='%d', delimiter=\",\")\n",
    "    \n",
    "    # Plot non-normalized confusion matrix\n",
    "    plot_title = 'Confusion matrix'\n",
    "    plotnorm_title = 'Confusion matrix (normalized)'\n",
    "    if condition:\n",
    "        plot_title += ' - Condition: %s'%condition\n",
    "        plotnorm_title += ' - Condition: %s'%condition\n",
    "    if weight:\n",
    "        plot_title += ' - Area weighted'\n",
    "        plotnorm_title += ' - Area weighted'\n",
    "    fig_cm = plt.figure(figsize=(15,10))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=classes,title=plot_title)\n",
    "    # Plot normalized confusion matrix\n",
    "    fig_cm_normal=plt.figure(figsize=(15,10))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True,title=plotnorm_title)\n",
    "    ## Set the path to the output\n",
    "    output_confmat_pdf = os.path.join(output_folder,\"confusionmatrix.pdf\")\n",
    "    output_confmatA_png = os.path.join(output_folder,\"confusionmatrixA.png\")\n",
    "    output_confmatB_png = os.path.join(output_folder,\"confusionmatrixB.png\")\n",
    "    # Export in PDF\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    pp = PdfPages(output_confmat_pdf)\n",
    "    pp.savefig(fig_cm)\n",
    "    pp.savefig(fig_cm_normal)\n",
    "    pp.close()\n",
    "    # Export in PNG\n",
    "    fig_cm.savefig(output_confmatA_png, format='png', dpi=300)\n",
    "    fig_cm_normal.savefig(output_confmatB_png, format='png', dpi=300)\n",
    "    \n",
    "    ##### Classification repport #####\n",
    "    # Define dataset to take into account\n",
    "    y_true = ref_list\n",
    "    y_pred = pred_list\n",
    "    class_label = classes\n",
    "    # Compute precision accuracy\n",
    "    if weight:\n",
    "        accuracy = accuracy_score(y_true, y_pred, normalize=True, sample_weight=area_list)\n",
    "    else:\n",
    "        accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    # Compute Cohen's Kappa\n",
    "    if weight:\n",
    "        cohen_kappa = cohen_kappa_score(y_true, y_pred, sample_weight=area_list)\n",
    "    else:\n",
    "        cohen_kappa = cohen_kappa_score(y_true, y_pred)    \n",
    "    # Compute f1-score\n",
    "    if weight:\n",
    "        f_1 = f1_score(y_true, y_pred, average='weighted', sample_weight=area_list)\n",
    "    else:\n",
    "        f_1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    # Compute 'classification report'\n",
    "    if weight:\n",
    "        classif_report = classification_report(y_true, y_pred, target_names=class_label, sample_weight=area_list)\n",
    "    else:\n",
    "        classif_report = classification_report(y_true, y_pred, target_names=class_label)    \n",
    "    # Save as .txt file\n",
    "    output = os.path.join(output_folder,\"classif_report.txt\")\n",
    "    f = open(output, 'w')\n",
    "    f.write(\"Folder name: '%s' \\n\"%output_folder)\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(\"Filter condition: '%s' \\n\"%condition)\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(\"----- Accuracy measures -----\\n\")\n",
    "    f.write(\"Overall Accuracy: \"+str(accuracy)+\"\\n\")\n",
    "    f.write(\"Cohen's Kappa: \"+str(cohen_kappa)+\"\\n\")\n",
    "    f.write(\"F1-score: \"+str(f_1)+\"\\n\")\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(\"----- Classification report -----\\n\")\n",
    "    f.write(classif_report)\n",
    "    f.close()\n",
    "    # Show file content\n",
    "    f = open(output,'r')\n",
    "    file_contents=f.read()\n",
    "    print(file_contents)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute accuracy measures - level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create connexion to postgres database\n",
    "con = create_pg_connexion(config_parameters)\n",
    "# Define output folder\n",
    "output_folder = os.path.join(config_parameters['validationfolder'],\"l1_validation\")\n",
    "# List of labels\n",
    "classes = ['1_ProductionPrimaire', '2_ProductionSecondaire', \n",
    "           '3_ProductionTertiaire', '4_Reseaux', '5_Residentiel', '6_Autres']\n",
    "# Compute accuracy measures \n",
    "GetAccuracyMeasure(con, schema='validation', table='valid_pred_ref', ref_column='ref_l1', pred_column='pred_l1', \n",
    "                   classes=classes, output_folder=output_folder, condition='certitude::int > 80', weight=False)\n",
    "# Close connexion to postgres database\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute accuracy measures - level 1 - surface weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create connexion to postgres database\n",
    "con = create_pg_connexion(config_parameters)\n",
    "# Define output folder\n",
    "output_folder = os.path.join(config_parameters['validationfolder'],\"l1_validation_weighted\")\n",
    "# List of labels\n",
    "classes = ['1_ProductionPrimaire', '2_ProductionSecondaire', \n",
    "           '3_ProductionTertiaire', '4_Reseaux', '5_Residentiel', '6_Autres']\n",
    "# Compute accuracy measures \n",
    "GetAccuracyMeasure(con, schema='validation', table='valid_pred_ref', ref_column='ref_l1', pred_column='pred_l1', \n",
    "                   classes=classes, output_folder=output_folder, condition='certitude::int > 80', weight=True)\n",
    "# Close connexion to postgres database\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute accuracy measures - level 1 - ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create connexion to postgres database\n",
    "con = create_pg_connexion(config_parameters)\n",
    "# Define output folder\n",
    "output_folder = os.path.join(config_parameters['validationfolder'],\"l1_validation_all\")\n",
    "# List of labels\n",
    "classes = ['1_ProductionPrimaire', '2_ProductionSecondaire', \n",
    "           '3_ProductionTertiaire', '4_Reseaux', '5_Residentiel', '6_Autres']\n",
    "# Compute accuracy measures \n",
    "GetAccuracyMeasure(con, schema='validation', table='valid_pred_ref', ref_column='ref_l1', pred_column='pred_l1', \n",
    "                   classes=classes, output_folder=output_folder, weight=False)\n",
    "# Close connexion to postgres database\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute accuracy measures - level 1 - surface weighted - ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create connexion to postgres database\n",
    "con = create_pg_connexion(config_parameters)\n",
    "# Define output folder\n",
    "output_folder = os.path.join(config_parameters['validationfolder'],\"l1_validation_weighted_all\")\n",
    "# List of labels\n",
    "classes = ['1_ProductionPrimaire', '2_ProductionSecondaire', \n",
    "           '3_ProductionTertiaire', '4_Reseaux', '5_Residentiel', '6_Autres']\n",
    "# Compute accuracy measures \n",
    "GetAccuracyMeasure(con, schema='validation', table='valid_pred_ref', ref_column='ref_l1', pred_column='pred_l1', \n",
    "                   classes=classes, output_folder=output_folder, weight=True)\n",
    "# Close connexion to postgres database\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute accuracy measures - level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create connexion to postgres database\n",
    "con = create_pg_connexion(config_parameters)\n",
    "# Define output folder\n",
    "output_folder = os.path.join(config_parameters['validationfolder'],\"l2_validation\")\n",
    "# List of labels\n",
    "classes = ['1_1','1_2','1_3',\n",
    "           '2_1','2_2','2_3','2_4',\n",
    "           '3_1','3_2','3_3','3_4',\n",
    "           '4_1','4_3',\n",
    "           '5_1','5_2','5_3',\n",
    "           '6_1','6_2','6_3','6_6']\n",
    "# Compute accuracy measures \n",
    "GetAccuracyMeasure(con, schema='validation', table='valid_pred_ref', ref_column='ref_l2', pred_column='pred_l2', \n",
    "                   classes=classes, output_folder=output_folder, \n",
    "                   condition='ref_l2 is not null and pred_l2 is not null and certitude::int > 80', weight=False)\n",
    "# Close connexion to postgres database\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute accuracy measures - level 2 - surface weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create connexion to postgres database\n",
    "con = create_pg_connexion(config_parameters)\n",
    "# Define output folder\n",
    "output_folder = os.path.join(config_parameters['validationfolder'],\"l2_validation_weighted\")\n",
    "# List of labels\n",
    "classes = ['1_1','1_2','1_3',\n",
    "           '2_1','2_2','2_3','2_4',\n",
    "           '3_1','3_2','3_3','3_4',\n",
    "           '4_1','4_3',\n",
    "           '5_1','5_2','5_3',\n",
    "           '6_1','6_2','6_3','6_6']\n",
    "# Compute accuracy measures \n",
    "GetAccuracyMeasure(con, schema='validation', table='valid_pred_ref', ref_column='ref_l2', pred_column='pred_l2', \n",
    "                   classes=classes, output_folder=output_folder, \n",
    "                   condition='ref_l2 is not null and pred_l2 is not null and certitude::int > 80', weight=True)\n",
    "# Close connexion to postgres database\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute accuracy measures - level 2 - ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create connexion to postgres database\n",
    "con = create_pg_connexion(config_parameters)\n",
    "# Define output folder\n",
    "output_folder = os.path.join(config_parameters['validationfolder'],\"l2_validation_all\")\n",
    "# List of labels\n",
    "classes = ['1_1','1_2','1_3',\n",
    "           '2_1','2_2','2_3','2_4',\n",
    "           '3_1','3_2','3_3','3_4',\n",
    "           '4_1','4_2','4_3',\n",
    "           '5_1','5_2','5_3',\n",
    "           '6_1','6_2','6_3','6_6']\n",
    "# Compute accuracy measures \n",
    "GetAccuracyMeasure(con, schema='validation', table='valid_pred_ref', ref_column='ref_l2', pred_column='pred_l2', \n",
    "                   classes=classes, output_folder=output_folder, \n",
    "                   condition='ref_l2 is not null and pred_l2 is not null', weight=False)\n",
    "# Close connexion to postgres database\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute accuracy measures - level 2 - surface weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create connexion to postgres database\n",
    "con = create_pg_connexion(config_parameters)\n",
    "# Define output folder\n",
    "output_folder = os.path.join(config_parameters['validationfolder'],\"l2_validation_weighted_all\")\n",
    "# List of labels\n",
    "classes = ['1_1','1_2','1_3',\n",
    "           '2_1','2_2','2_3','2_4',\n",
    "           '3_1','3_2','3_3','3_4',\n",
    "           '4_1','4_2','4_3',\n",
    "           '5_1','5_2','5_3',\n",
    "           '6_1','6_2','6_3','6_6']\n",
    "# Compute accuracy measures \n",
    "GetAccuracyMeasure(con, schema='validation', table='valid_pred_ref', ref_column='ref_l2', pred_column='pred_l2', \n",
    "                   classes=classes, output_folder=output_folder, \n",
    "                   condition='ref_l2 is not null and pred_l2 is not null', weight=True)\n",
    "# Close connexion to postgres database\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified random selection of cadastral plots for visual validation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Name of the new table that will be created\n",
    "strat_random_table = \"valid_lu_stratrand\"\n",
    "# Number of item per category\n",
    "num_percat = 25"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Time at starting\n",
    "begintime = time.time()  \n",
    "con = create_pg_connexion(config_parameters)\n",
    "cursor = con.cursor()\n",
    "# Drop table if exits\n",
    "cursor.execute(\"DROP TABLE IF EXISTS validation.%s\"%strat_random_table)\n",
    "# Subquery\n",
    "subquery = \"SELECT geom, capakey, classif_step1, all_hilucs, \"\n",
    "subquery += \"ROW_NUMBER() OVER (Partition By classif_step1 Order By random()) AS RowNo \"\n",
    "subquery += \"FROM results.%s\"%origin_table\n",
    "# Main query\n",
    "mainquery = \"SELECT geom, capakey, all_hilucs, classif_step1 \"\n",
    "mainquery += \"FROM (%s) AS a WHERE rowNo <=%s\"%(subquery,num_percat)\n",
    "# Create table query\n",
    "cursor.execute(\"CREATE TABLE validation.%s AS (%s);\"%(strat_random_table,mainquery))\n",
    "# Add columns\n",
    "cursor.execute(\"ALTER TABLE validation.%s ADD COLUMN lu_visual_valid varchar\"%strat_random_table)\n",
    "cursor.execute(\"ALTER TABLE validation.%s ADD COLUMN comment varchar\"%strat_random_table)\n",
    "# Make the changes to the database persistent\n",
    "con.commit()\n",
    "# Close connection with database\n",
    "cursor.close()\n",
    "# Close connexion to postgres database\n",
    "con.close()\n",
    "## Print processing time\n",
    "print print_processing_time(begintime, \"Processing achieved in \")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Create connexion to postgres database\n",
    "con = create_pg_connexion(config_parameters)\n",
    "# Display header\n",
    "df = display_header(con, 'validation', strat_random_table, row_num=10)\n",
    "# Close connexion to postgres database\n",
    "con.close()\n",
    "# Display dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
